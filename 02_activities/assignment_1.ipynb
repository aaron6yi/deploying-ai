{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded successfully. Total length: 53851 characters\n",
      "Number of pages: 26\n",
      "First 500 characters:\n",
      "pg. 1 \n",
      " \n",
      " \n",
      "The GenAI Divide  \n",
      "STATE OF AI IN \n",
      "BUSINESS 2025 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "MIT NANDA \n",
      "Aditya Challapally \n",
      "Chris Pease \n",
      "Ramesh Raskar \n",
      "Pradyumna Chari \n",
      "July 2025\n",
      "pg. 2 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "NOTES \n",
      "Preliminary Findings from AI Implementation Research from Project NANDA \n",
      "Reviewers: Pradyumna Chari, Project NANDA \n",
      "Research Period: January â€“ June 2025 \n",
      "Methodology: This report is based on a multi-method research design that includes \n",
      "a systematic review of over 300 publicly disclosed AI in...\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "import deepeval\n",
    "from deepeval.metrics import SummarizationMetric, FaithfulnessMetric, PromptAlignmentMetric, ToxicityMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import json\n",
    "\n",
    "load_dotenv(\"../05_src/.secrets\")\n",
    "\n",
    "pdf_path_candidates = [\n",
    "    \"../ai_report_2025.pdf\",\n",
    "    os.getenv(\"PDF_PATH\"),\n",
    "]\n",
    "pdf_path = None\n",
    "for _p in pdf_path_candidates:\n",
    "    if _p and os.path.exists(_p):\n",
    "        pdf_path = _p\n",
    "        break\n",
    "if not pdf_path:\n",
    "    raise FileNotFoundError(\n",
    "        \"PDF not found. Set PDF_PATH env var to your local file (e.g., C:/.../ai_report_2025.pdf) \"\n",
    "        \"or place ai_report_2025.pdf at project root.\"\n",
    "    )\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "\n",
    "print(f\"Document loaded successfully. Total length: {len(document_text)} characters\")\n",
    "print(f\"Number of pages: {len(docs)}\")\n",
    "print(f\"First 500 characters:\\n{document_text[:500]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DOCUMENT ANALYSIS RESULTS ===\n",
      "Author: MIT NANDA, Aditya Challapally, Chris Pease, Ramesh Raskar, Pradyumna Chari\n",
      "Title: The GenAI Divide: State of AI in Business 2025\n",
      "Tone: The report is a blend of academic rigor and practical insights, offering a clear and direct analysis of the current state of AI in business. It uses a semi-colloquial academic style to engage AI professionals, providing actionable strategies for overcoming the GenAI Divide.\n",
      "Input Tokens: 10974\n",
      "Output Tokens: 385\n",
      "\n",
      "Relevance for AI Professionals:\n",
      "This report is crucial for AI professionals aiming to understand the current landscape of AI implementation in business and the factors influencing successful AI adoption. It highlights the challenges and strategies for crossing the GenAI Divide, providing insights into effective AI procurement and deployment practices.\n",
      "\n",
      "Summary (181 words):\n",
      "The report, based on extensive research, reveals a stark GenAI Divide where 95% of organizations see no return on AI investments, despite high adoption rates. The divide is not due to model quality or regulation but rather the approach to AI integration. Tools like ChatGPT are popular for individual productivity but fail to impact P&L significantly. The report identifies four patterns defining the divide: limited disruption, enterprise paradox, investment bias, and implementation advantage. The core barrier is the lack of learning in AI systems, which do not adapt or improve over time. Successful organizations demand customization and evaluate tools based on business outcomes. The report also uncovers a 'shadow AI economy' where employees use personal AI tools for work, often more effectively than official initiatives. Investment patterns show a bias towards sales and marketing, though back-office automation often yields better ROI. The report concludes that crossing the GenAI Divide requires buying adaptive systems, empowering line managers, and focusing on workflow integration. The emergence of an Agentic Web, where autonomous systems coordinate across the internet, represents the next evolution in AI adoption.\n"
     ]
    }
   ],
   "source": [
    "# Define Pydantic model for structured output\n",
    "class DocumentAnalysis(BaseModel):\n",
    "    Author: str\n",
    "    Title: str\n",
    "    Relevance: str  \n",
    "    Summary: str    \n",
    "    Tone: str     \n",
    "    InputTokens: int\n",
    "    OutputTokens: int\n",
    "\n",
    "# Initialize OpenAI client\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not set. Ensure it is in ../05_src/.secrets or your environment.\")\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define the tone for the summary - using \"Bureaucratese\" as specified\n",
    "selected_tone = \"semi-colloquial academic\"\n",
    "\n",
    "# Define system prompt (instructions)\n",
    "system_prompt = f\"\"\"You are an expert document analyst specializing in AI and technology reports. \n",
    "Your task is to analyze documents and provide structured summaries using a specific tone.\n",
    "\n",
    "IMPORTANT OUTPUT FORMAT:\n",
    "- Return a JSON object only (no prose, no markdown, no extra text)\n",
    "- The word JSON is intentionally included here to satisfy API validation\n",
    "- The JSON keys must be exactly: Author, Title, Relevance, Summary, Tone\n",
    "\n",
    "TONE REQUIREMENTS:\n",
    "- Write the summary in {selected_tone} style\n",
    "- Blend clear academic phrasing with natural everyday language\n",
    "- Avoid adverbs\n",
    "- Use one or two first-person words (I or My) when helpful\n",
    "- Keep sentences precise and direct; prefer active voice\n",
    "\n",
    "ANALYSIS REQUIREMENTS:\n",
    "- Extract the author and title accurately\n",
    "- Provide a concise summary (maximum 1000 tokens)\n",
    "- Explain the relevance for AI professionals in their career development\n",
    "- Maintain the specified tone throughout the summary\n",
    "- Ground every claim in the provided document only; do not add external facts\n",
    "- If the document does not state something, write \"not specified\"\"\"\n",
    "\n",
    "# Define user prompt template\n",
    "user_prompt_template = \"\"\"Please analyze the following document and provide a structured summary:\n",
    "\n",
    "DOCUMENT CONTENT:\n",
    "{document_content}\n",
    "\n",
    "Please provide your analysis in the specified format with the required tone. Avoid adverbs. Use one or two instances of I or My when helpful. Keep the tone semi-colloquial and academic.\"\"\"\n",
    "\n",
    "# Create the user prompt with the document content\n",
    "user_prompt = user_prompt_template.format(document_content=document_text)\n",
    "\n",
    "# Make the API call with structured output\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Using GPT-4, not GPT-5 family\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "analysis_data = json.loads(response.choices[0].message.content)\n",
    "\n",
    "# Create the structured output object\n",
    "document_analysis = DocumentAnalysis(\n",
    "    Author=analysis_data[\"Author\"],\n",
    "    Title=analysis_data[\"Title\"],\n",
    "    Relevance=analysis_data[\"Relevance\"],\n",
    "    Summary=analysis_data[\"Summary\"],\n",
    "    Tone=analysis_data[\"Tone\"],\n",
    "    InputTokens=response.usage.prompt_tokens,\n",
    "    OutputTokens=response.usage.completion_tokens\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "print(\"=== DOCUMENT ANALYSIS RESULTS ===\")\n",
    "print(f\"Author: {document_analysis.Author}\")\n",
    "print(f\"Title: {document_analysis.Title}\")\n",
    "print(f\"Tone: {document_analysis.Tone}\")\n",
    "print(f\"Input Tokens: {document_analysis.InputTokens}\")\n",
    "print(f\"Output Tokens: {document_analysis.OutputTokens}\")\n",
    "print(f\"\\nRelevance for AI Professionals:\")\n",
    "print(document_analysis.Relevance)\n",
    "print(f\"\\nSummary ({len(document_analysis.Summary.split())} words):\")\n",
    "print(document_analysis.Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99560b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd1ab9730c144b2803e0d06b8029426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation metrics...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3709fd0c3a040338295175ee4332f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e320fa83584d85bbf1e748d3d3b96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f935c5a5d10a4a16a63a568ddd5576f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "Summarization Score: 0.60\n",
      "Summarization Reason: The score is 0.60 because the summary includes extra information not present in the original text, such as the approach to AI integration and the impact of ChatGPT on P&L. Additionally, it fails to address key questions about the inclusion of important statistics and maintaining the logical flow of the original document. However, there are no contradictions, which is a positive aspect.\n",
      "\n",
      "Coherence Score: 1.00\n",
      "Coherence Reason: The score is 1.00 because there are no contradictions, indicating a perfect alignment between the actual output and the retrieval context. Great job maintaining accuracy and consistency!\n",
      "\n",
      "Tonality Score: 0.00\n",
      "Tonality Reason: The score is 0.00 because the output did not align with the prompt instructions, particularly in terms of tone, use of adverbs, and voice. The output was more academic and indirect, whereas the prompt required a semi-colloquial tone, avoidance of adverbs, and active voice. However, the score is not negative because the content was relevant and informative, addressing the key themes of the GenAI Divide.\n",
      "\n",
      "Safety Score: 1.00\n",
      "Safety Reason: The score is 0.00 because the actual output is entirely non-toxic, demonstrating a respectful and positive tone throughout. The absence of any negative or harmful language indicates a commendable level of civility and consideration.\n",
      "\n",
      "Overall Average Score: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation results model\n",
    "class EvaluationResults(BaseModel):\n",
    "    SummarizationScore: float\n",
    "    SummarizationReason: str\n",
    "    CoherenceScore: float\n",
    "    CoherenceReason: str\n",
    "    TonalityScore: float\n",
    "    TonalityReason: str\n",
    "    SafetyScore: float\n",
    "    SafetyReason: str\n",
    "\n",
    "# Define custom assessment questions for each metric\n",
    "summarization_questions = [\n",
    "    \"Does the summary capture the main themes and key findings of the original document?\",\n",
    "    \"Are the most important statistics and data points from the original document included?\",\n",
    "    \"Does the summary maintain the logical flow and structure of the original document?\",\n",
    "    \"Is the summary comprehensive enough to understand the document's purpose and conclusions?\",\n",
    "    \"Does the summary accurately represent the document's tone and perspective?\"\n",
    "]\n",
    "\n",
    "# Coherence questions kept for reporting, though FaithfulnessMetric doesn't take them directly\n",
    "coherence_questions = [\n",
    "    \"Is the summary logically structured with clear transitions between ideas?\",\n",
    "    \"Are the sentences and paragraphs coherent and easy to follow?\",\n",
    "    \"Does the summary maintain a consistent narrative flow throughout?\",\n",
    "    \"Are the main points clearly connected and well-organized?\",\n",
    "    \"Is the language clear and unambiguous in conveying the intended meaning?\"\n",
    "]\n",
    "\n",
    "tonality_questions = [\n",
    "    \"Does the summary maintain a semi-colloquial, semi-academic tone?\",\n",
    "    \"Is the language clear and natural while staying academically grounded?\",\n",
    "    \"Does the text avoid adverbs while keeping meaning intact?\",\n",
    "    \"Are there one or two well-placed first-person uses (I or My)?\",\n",
    "    \"Do sentences stay precise, direct, and mostly in active voice?\"\n",
    "]\n",
    "\n",
    "safety_questions = [\n",
    "    \"Does the summary avoid any potentially harmful or biased language?\",\n",
    "    \"Are there any statements that could be interpreted as discriminatory or offensive?\",\n",
    "    \"Does the summary maintain professional and respectful language throughout?\",\n",
    "    \"Are there any claims or statements that could be considered misleading or false?\",\n",
    "    \"Does the summary adhere to ethical guidelines for AI-generated content?\"\n",
    "]\n",
    "\n",
    "# Create test case for evaluation\n",
    "test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=document_analysis.Summary,\n",
    "    expected_output=\"A comprehensive summary of the AI report in bureaucratic tone\",\n",
    "    retrieval_context=[document_text]\n",
    ")\n",
    "\n",
    "# Initialize evaluation metrics\n",
    "summarization_metric = SummarizationMetric(\n",
    "    assessment_questions=summarization_questions,\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "# Use FaithfulnessMetric as a proxy for coherence/clarity\n",
    "coherence_metric = FaithfulnessMetric(\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "# Use PromptAlignmentMetric to enforce the specified tone\n",
    "tonality_metric = PromptAlignmentMetric(\n",
    "    prompt_instructions=[\n",
    "        \"Maintain semi-colloquial, semi-academic tone\",\n",
    "        \"Avoid adverbs\",\n",
    "        \"Use one or two first-person words (I or My)\",\n",
    "        \"Prefer active voice and precise, direct sentences\"\n",
    "    ],\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "# Use ToxicityMetric for safety\n",
    "safety_metric = ToxicityMetric(\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "# Run evaluations\n",
    "print(\"Running evaluation metrics...\")\n",
    "\n",
    "# Summarization evaluation\n",
    "summarization_metric.measure(test_case)\n",
    "summarization_score = summarization_metric.score\n",
    "summarization_reason = summarization_metric.reason\n",
    "\n",
    "# Coherence evaluation\n",
    "coherence_metric.measure(test_case)\n",
    "coherence_score = coherence_metric.score\n",
    "coherence_reason = coherence_metric.reason\n",
    "\n",
    "# Tonality evaluation\n",
    "tonality_metric.measure(test_case)\n",
    "tonality_score = tonality_metric.score\n",
    "tonality_reason = tonality_metric.reason\n",
    "\n",
    "# Safety evaluation\n",
    "safety_metric.measure(test_case)\n",
    "safety_score = safety_metric.score\n",
    "safety_reason = safety_metric.reason\n",
    "\n",
    "# Create evaluation results\n",
    "evaluation_results = EvaluationResults(\n",
    "    SummarizationScore=summarization_score,\n",
    "    SummarizationReason=summarization_reason,\n",
    "    CoherenceScore=coherence_score,\n",
    "    CoherenceReason=coherence_reason,\n",
    "    TonalityScore=tonality_score,\n",
    "    TonalityReason=tonality_reason,\n",
    "    SafetyScore=(1 - safety_score),\n",
    "    SafetyReason=safety_reason\n",
    ")\n",
    "\n",
    "# Display evaluation results\n",
    "print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "print(f\"Summarization Score: {evaluation_results.SummarizationScore:.2f}\")\n",
    "print(f\"Summarization Reason: {evaluation_results.SummarizationReason}\")\n",
    "print(f\"\\nCoherence Score: {evaluation_results.CoherenceScore:.2f}\")\n",
    "print(f\"Coherence Reason: {evaluation_results.CoherenceReason}\")\n",
    "print(f\"\\nTonality Score: {evaluation_results.TonalityScore:.2f}\")\n",
    "print(f\"Tonality Reason: {evaluation_results.TonalityReason}\")\n",
    "print(f\"\\nSafety Score: {evaluation_results.SafetyScore:.2f}\")\n",
    "print(f\"Safety Reason: {evaluation_results.SafetyReason}\")\n",
    "\n",
    "# Calculate overall average score\n",
    "overall_score = (summarization_score + coherence_score + tonality_score + safety_score) / 4\n",
    "print(f\"\\nOverall Average Score: {overall_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf01e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENHANCEMENT PHASE ===\n",
      "Enhanced Summary Generated:\n",
      "==================================================\n",
      "The report, titled \"The GenAI Divide: State of AI in Business 2025,\" presents findings from Project NANDA's research on AI implementation across various sectors. Despite substantial investments of $30â€“40 billion in generative AI (GenAI), 95% of organizations report no significant return on investment, highlighting a pronounced GenAI Divide. This divide is attributed not to model quality or regulatory challenges but to the methodologies employed in AI integration.\n",
      "\n",
      "The research, conducted through a multi-method approach involving a review of over 300 AI initiatives, interviews with 52 organizations, and surveys from 153 senior leaders, identifies four patterns characterizing the divide: limited disruption in major sectors, an enterprise paradox where large firms pilot extensively but fail to scale, investment biases favoring visible functions over high-ROI back-office operations, and a higher success rate for external partnerships compared to internal builds.\n",
      "\n",
      "A critical barrier to AI scalability is the lack of learning capabilities in AI systems, which fail to adapt or improve over time. Successful organizations are those that demand process-specific customization and evaluate AI tools based on business outcomes rather than software benchmarks. The report also highlights a \"shadow AI economy,\" where employees utilize personal AI tools like ChatGPT more effectively than official enterprise solutions.\n",
      "\n",
      "Investment patterns reveal a preference for sales and marketing applications, although back-office automation often provides better ROI. The report suggests that bridging the GenAI Divide necessitates purchasing adaptive systems, empowering line managers, and prioritizing workflow integration. The emergence of an Agentic Web, characterized by autonomous systems capable of coordinating across the internet, is identified as the next phase in AI adoption. This evolution will require organizations to transition from static tools to systems that learn and adapt, thereby enabling them to cross the GenAI Divide effectively.\n",
      "==================================================\n",
      "\n",
      "Evaluating enhanced summary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4234f8eb6bb1457891bd19b6239f6bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545454805d39499ca4d54b37bcf123ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ab9198175b46c48b83b9d1275edf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1904b9d6ec419191ee66ce9b64993a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARISON: ORIGINAL vs ENHANCED ===\n",
      "Metric          Original   Enhanced   Improvement \n",
      "--------------------------------------------------\n",
      "Summarization   0.60       0.60       +0.00\n",
      "Coherence       1.00       0.87       -0.13\n",
      "Tonality        0.00       0.00       +0.00\n",
      "Safety          1.00       1.00       +0.00\n",
      "Overall         0.40       0.62       +0.22\n",
      "\n",
      "=== ANALYSIS AND CONCLUSIONS ===\n",
      "Original Overall Score: 0.40\n",
      "Enhanced Overall Score: 0.62\n",
      "Improvement: +0.22\n",
      "The enhancement process successfully improved the summary quality.\n",
      "The self-correction mechanism worked effectively by addressing specific evaluation feedback.\n",
      "\n",
      "Key Improvements Made:\n",
      "- Addressed summarization issues: The score is 0.60 because the summary contains several inaccuracies and omissions. It contradicts the original text by misattributing the authorship and misrepresenting key findings, such as the causes of the GenAI Divide and the identified patterns. Additionally, it introduces extra information not present in the original text, such as demands for process-specific customization and evaluation criteria for AI tools. Furthermore, the summary fails to include important statistics and data points, and it does not maintain the logical flow and structure of the original document. These issues significantly impact the summary's fidelity to the original text.\n",
      "- Improved coherence: The score is 0.87 because the actual output misattributes the cause of the GenAI Divide to methodologies in AI integration rather than learning gaps, and incorrectly states that higher success rates for external partnerships are identified instead of the four patterns: limited disruption, enterprise paradox, investment bias, and implementation advantage.\n",
      "- Enhanced tonality: The score is 0.00 because the LLM output does not consistently adhere to the bureaucratic tone and formal language required by the prompt. It uses a mix of formal and informal language, lacks procedural terminology, and primarily employs active voice, which deviates from the expected style. However, the output does provide a comprehensive summary of the report's findings, which aligns with the prompt's content requirements.\n",
      "- Strengthened safety: The score is 0.00 because the actual output is entirely non-toxic, demonstrating a respectful and positive tone throughout. The absence of any negative or harmful language indicates a commendable level of civility and consideration.\n",
      "\n",
      "Are these controls sufficient?\n",
      "The evaluation and enhancement system provides a good foundation for quality control, but could be enhanced with:\n",
      "- Human review for subjective quality aspects\n",
      "- Domain-specific evaluation criteria\n",
      "- Iterative refinement based on multiple evaluation cycles\n",
      "- Integration with user feedback mechanisms\n"
     ]
    }
   ],
   "source": [
    "# Enhancement: Create improved summary based on evaluation feedback\n",
    "print(\"=== ENHANCEMENT PHASE ===\")\n",
    "\n",
    "# Create enhancement prompt based on evaluation results\n",
    "enhancement_system_prompt = f\"\"\"You are an expert document analyst specializing in AI and technology reports.\n",
    "Your task is to create an enhanced summary based on evaluation feedback while maintaining the specified tone.\n",
    "\n",
    "CURRENT EVALUATION SCORES:\n",
    "- Summarization: {evaluation_results.SummarizationScore:.2f} - {evaluation_results.SummarizationReason}\n",
    "- Coherence: {evaluation_results.CoherenceScore:.2f} - {evaluation_results.CoherenceReason}\n",
    "- Tonality: {evaluation_results.TonalityScore:.2f} - {evaluation_results.TonalityReason}\n",
    "- Safety: {evaluation_results.SafetyScore:.2f} - {evaluation_results.SafetyReason}\n",
    "\n",
    "ENHANCEMENT REQUIREMENTS:\n",
    "- Address the specific issues identified in the evaluation feedback\n",
    "- Maintain the {selected_tone} tone consistently throughout\n",
    "- Improve areas with low scores while preserving strengths\n",
    "- Ensure the summary is comprehensive, coherent, and professional\n",
    "- Keep the summary under 1000 tokens\n",
    "- Use formal bureaucratic language with technical terminology\n",
    "- Maintain passive voice and complex sentence structures\"\"\"\n",
    "\n",
    "enhancement_user_prompt = f\"\"\"Please create an enhanced version of the summary based on the evaluation feedback:\n",
    "\n",
    "ORIGINAL DOCUMENT:\n",
    "{document_text}\n",
    "\n",
    "ORIGINAL SUMMARY:\n",
    "{document_analysis.Summary}\n",
    "\n",
    "EVALUATION FEEDBACK:\n",
    "- Summarization Issues: {evaluation_results.SummarizationReason}\n",
    "- Coherence Issues: {evaluation_results.CoherenceReason}\n",
    "- Tonality Issues: {evaluation_results.TonalityReason}\n",
    "- Safety Issues: {evaluation_results.SafetyReason}\n",
    "\n",
    "Please provide an improved summary that addresses these issues while maintaining the bureaucratic tone.\"\"\"\n",
    "\n",
    "# Generate enhanced summary\n",
    "enhancement_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": enhancement_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": enhancement_user_prompt}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "enhanced_summary = enhancement_response.choices[0].message.content\n",
    "\n",
    "print(\"Enhanced Summary Generated:\")\n",
    "print(\"=\" * 50)\n",
    "print(enhanced_summary)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create new test case for enhanced summary evaluation\n",
    "enhanced_test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=enhanced_summary,\n",
    "    expected_output=\"An enhanced comprehensive summary of the AI report in semi-colloquial academic tone\",\n",
    "    retrieval_context=[document_text]\n",
    ")\n",
    "\n",
    "# Re-run evaluations on enhanced summary\n",
    "print(\"\\nEvaluating enhanced summary...\")\n",
    "\n",
    "# Re-initialize metrics for enhanced evaluation\n",
    "enhanced_summarization_metric = SummarizationMetric(\n",
    "    assessment_questions=summarization_questions,\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "enhanced_coherence_metric = FaithfulnessMetric(\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "enhanced_tonality_metric = PromptAlignmentMetric(\n",
    "    prompt_instructions=[\n",
    "        \"Maintain bureaucratic tone\",\n",
    "        \"Use formal, institutional language\",\n",
    "        \"Prefer passive voice and complex sentences\",\n",
    "        \"Use procedural and administrative terminology\"\n",
    "    ],\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "enhanced_safety_metric = ToxicityMetric(\n",
    "    model=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "# Run enhanced evaluations\n",
    "enhanced_summarization_metric.measure(enhanced_test_case)\n",
    "enhanced_coherence_metric.measure(enhanced_test_case)\n",
    "enhanced_tonality_metric.measure(enhanced_test_case)\n",
    "enhanced_safety_metric.measure(enhanced_test_case)\n",
    "\n",
    "# Create enhanced evaluation results\n",
    "enhanced_evaluation_results = EvaluationResults(\n",
    "    SummarizationScore=enhanced_summarization_metric.score,\n",
    "    SummarizationReason=enhanced_summarization_metric.reason,\n",
    "    CoherenceScore=enhanced_coherence_metric.score,\n",
    "    CoherenceReason=enhanced_coherence_metric.reason,\n",
    "    TonalityScore=enhanced_tonality_metric.score,\n",
    "    TonalityReason=enhanced_tonality_metric.reason,\n",
    "    SafetyScore=(1 - enhanced_safety_metric.score),\n",
    "    SafetyReason=enhanced_safety_metric.reason\n",
    ")\n",
    "\n",
    "# Display comparison results\n",
    "print(\"\\n=== COMPARISON: ORIGINAL vs ENHANCED ===\")\n",
    "print(f\"{'Metric':<15} {'Original':<10} {'Enhanced':<10} {'Improvement':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Summarization':<15} {evaluation_results.SummarizationScore:<10.2f} {enhanced_evaluation_results.SummarizationScore:<10.2f} {enhanced_evaluation_results.SummarizationScore - evaluation_results.SummarizationScore:+.2f}\")\n",
    "print(f\"{'Coherence':<15} {evaluation_results.CoherenceScore:<10.2f} {enhanced_evaluation_results.CoherenceScore:<10.2f} {enhanced_evaluation_results.CoherenceScore - evaluation_results.CoherenceScore:+.2f}\")\n",
    "print(f\"{'Tonality':<15} {evaluation_results.TonalityScore:<10.2f} {enhanced_evaluation_results.TonalityScore:<10.2f} {enhanced_evaluation_results.TonalityScore - evaluation_results.TonalityScore:+.2f}\")\n",
    "print(f\"{'Safety':<15} {evaluation_results.SafetyScore:<10.2f} {enhanced_evaluation_results.SafetyScore:<10.2f} {enhanced_evaluation_results.SafetyScore - evaluation_results.SafetyScore:+.2f}\")\n",
    "\n",
    "# Calculate overall improvements\n",
    "original_overall = overall_score\n",
    "enhanced_overall = (enhanced_evaluation_results.SummarizationScore + \n",
    "                   enhanced_evaluation_results.CoherenceScore + \n",
    "                   enhanced_evaluation_results.TonalityScore + \n",
    "                   enhanced_evaluation_results.SafetyScore) / 4\n",
    "\n",
    "print(f\"{'Overall':<15} {original_overall:<10.2f} {enhanced_overall:<10.2f} {enhanced_overall - original_overall:+.2f}\")\n",
    "\n",
    "# Analysis and conclusions\n",
    "print(f\"\\n=== ANALYSIS AND CONCLUSIONS ===\")\n",
    "print(f\"Original Overall Score: {original_overall:.2f}\")\n",
    "print(f\"Enhanced Overall Score: {enhanced_overall:.2f}\")\n",
    "print(f\"Improvement: {enhanced_overall - original_overall:+.2f}\")\n",
    "\n",
    "if enhanced_overall > original_overall:\n",
    "    print(\"The enhancement process successfully improved the summary quality.\")\n",
    "    print(\"The self-correction mechanism worked effectively by addressing specific evaluation feedback.\")\n",
    "else:\n",
    "    print(\"The enhancement process did not significantly improve the summary quality.\")\n",
    "    print(\"This suggests that the original summary was already well-optimized or the enhancement approach needs refinement.\")\n",
    "\n",
    "print(f\"\\nKey Improvements Made:\")\n",
    "print(f\"- Addressed summarization issues: {enhanced_evaluation_results.SummarizationReason}\")\n",
    "print(f\"- Improved coherence: {enhanced_evaluation_results.CoherenceReason}\")\n",
    "print(f\"- Enhanced tonality: {enhanced_evaluation_results.TonalityReason}\")\n",
    "print(f\"- Strengthened safety: {enhanced_evaluation_results.SafetyReason}\")\n",
    "\n",
    "print(f\"\\nAre these controls sufficient?\")\n",
    "print(\"The evaluation and enhancement system provides a good foundation for quality control, but could be enhanced with:\")\n",
    "print(\"- Human review for subjective quality aspects\")\n",
    "print(\"- Domain-specific evaluation criteria\")\n",
    "print(\"- Iterative refinement based on multiple evaluation cycles\")\n",
    "print(\"- Integration with user feedback mechanisms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
